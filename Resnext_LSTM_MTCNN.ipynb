{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMARRr7eJ0HGfD7bO0hZzad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Najemeddinebessaoud/DeepFake_tf/blob/master/Resnext_LSTM_MTCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HleT7sBEE9bO"
      },
      "outputs": [],
      "source": [
        "# Restart the runtime\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qev1RXJ_FQip",
        "outputId": "7c88312d-0f83-4f76-b0bd-996808b49ebc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ESnMrX6YFT7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch_xla for TPU support\n",
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77LZEq0BFcoN",
        "outputId": "bfd7007b-bd72-4f41-8760-06137dbee3d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: torch_xla-2.0-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGWl7sykFjZH",
        "outputId": "414125a5-e832-4e84-8531-730f3171ac8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow-cpu\n",
            "  Downloading tensorflow_cpu-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-cpu)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow-cpu)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-cpu)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow-cpu)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow-cpu)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-cpu)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-cpu)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow-cpu) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow-cpu) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow-cpu) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow-cpu)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow-cpu)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow-cpu) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\n",
            "Downloading tensorflow_cpu-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (251.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.8/251.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow-cpu\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-cpu-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_xla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4adFX_iFmVk",
        "outputId": "e8755bce-4453-4e90-9fe5-291e660bf0c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_xla in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS6kZ5CZFpU_",
        "outputId": "cd774423-9d07-4d67-a9f8-9a04f0eeaf13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.11.0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y lz4 mtcnn\n",
        "!pip install lz4 mtcnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIS3ZR1rFxIe",
        "outputId": "8b90e0b8-1d75-40b4-dd44-d3f7e3f3516c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping lz4 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping mtcnn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting lz4\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting mtcnn\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (1.4.2)\n",
            "Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4, mtcnn\n",
            "Successfully installed lz4-4.4.4 mtcnn-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "# Check if TPU is available\n",
        "device = xm.xla_device()\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpiXQ9ySF0_b",
        "outputId": "25949eaa-58da-4e86-dce6-bf63bd94d7f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: xla:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import random\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "PhqH3qmmGAnl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mtcnn import MTCNN\n",
        "# Initialize MTCNN detector\n",
        "detector = MTCNN()"
      ],
      "metadata": {
        "id": "f5-fpGZxGDzV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset & Preprocessing"
      ],
      "metadata": {
        "id": "zfF9Ot3gGUCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_extract(path):\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    success = 1\n",
        "    frame_count = 0\n",
        "    while success:\n",
        "        success, image = vidObj.read()\n",
        "        if success:\n",
        "            print(f\"Extracted frame {frame_count} from {path}\")  # Debug: Print frame count\n",
        "            yield image\n",
        "            frame_count += 1\n",
        "    print(f\"Total frames extracted from {path}: {frame_count}\")  # Debug: Print total frames"
      ],
      "metadata": {
        "id": "v-zrnGI5GcWE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_video(vid_path, train_transforms):\n",
        "    frames = []\n",
        "    for i, frame in enumerate(frame_extract(vid_path)):\n",
        "        # Detect faces in the frame using MTCNN\n",
        "        faces = detector.detect_faces(frame)\n",
        "\n",
        "        # Only process frames that contain at least one face\n",
        "        if len(faces) > 0:\n",
        "            # Apply transformations to the frame\n",
        "            transformed_frame = train_transforms(frame)\n",
        "            frames.append(transformed_frame)\n",
        "\n",
        "        # Stop after collecting 50 frames with faces\n",
        "        if len(frames) == 60:\n",
        "            break\n",
        "\n",
        "    # Stack the frames into a tensor\n",
        "    frames = torch.stack(frames)\n",
        "    return frames"
      ],
      "metadata": {
        "id": "Iw3xkSMvG351"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_names, labels, sequence_length=10, transform=None, im_size=112):\n",
        "        self.video_names = video_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "        self.im_size = im_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        print(f\"Processing video: {video_path}\")\n",
        "        frames = []\n",
        "        for i, frame in enumerate(self.frame_extract(video_path)):\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)  # Apply transforms\n",
        "            frames.append(frame)\n",
        "            if len(frames) == self.sequence_length:\n",
        "                break\n",
        "\n",
        "        # Handle cases where no frames are extracted\n",
        "        if len(frames) == 0:\n",
        "            print(f\"Warning: No frames extracted from {video_path}. Using zero-padded frames.\")\n",
        "            frames = [torch.zeros(3, self.im_size, self.im_size) for _ in range(self.sequence_length)]\n",
        "\n",
        "        # Pad frames if necessary\n",
        "        if len(frames) < self.sequence_length:\n",
        "            padding = [torch.zeros_like(frames[0]) for _ in range(self.sequence_length - len(frames))]\n",
        "            frames.extend(padding)\n",
        "\n",
        "        frames = torch.stack(frames)  # Stack frames into a tensor\n",
        "        label = self.labels[idx]\n",
        "        return frames, label\n",
        "\n",
        "    def frame_extract(self, path):\n",
        "        vidObj = cv2.VideoCapture(path)\n",
        "        if not vidObj.isOpened():\n",
        "            print(f\"Warning: Unable to open video {path}. Skipping.\")\n",
        "            return\n",
        "        success = 1\n",
        "        frame_count = 0\n",
        "        while success:\n",
        "            success, image = vidObj.read()\n",
        "            if success:\n",
        "                yield image\n",
        "                frame_count += 1\n",
        "        print(f\"Total frames extracted from {path}: {frame_count}\")"
      ],
      "metadata": {
        "id": "W4nvGB0PG6gD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_videos_and_labels(fake_folder, real_folder):\n",
        "    fake_videos = glob.glob(os.path.join(fake_folder, '*.mp4'))\n",
        "    print(f\"Found {len(fake_videos)} fake videos in {fake_folder}\")\n",
        "\n",
        "    real_videos = glob.glob(os.path.join(real_folder, '*.mp4'))\n",
        "    print(f\"Found {len(real_videos)} real videos in {real_folder}\")\n",
        "\n",
        "    if len(fake_videos) == 0 and len(real_videos) == 0:\n",
        "        raise ValueError(f\"No videos found in {fake_folder} or {real_folder}. Please check the paths.\")\n",
        "\n",
        "    video_files = fake_videos + real_videos\n",
        "    labels = [0] * len(fake_videos) + [1] * len(real_videos)\n",
        "\n",
        "    combined = list(zip(video_files, labels))\n",
        "    random.shuffle(combined)\n",
        "    if len(combined) == 0:\n",
        "        raise ValueError(\"No videos found after combining fake and real folders.\")\n",
        "    video_files[:], labels[:] = zip(*combined)\n",
        "\n",
        "    return video_files, labels"
      ],
      "metadata": {
        "id": "BZ3ws7yWG9-8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "def load_videos_and_labels(fake_folder, real_folder):\n",
        "    # Find all .mp4 files in the fake folder\n",
        "    fake_videos = glob.glob(os.path.join(fake_folder, '*.mp4'))\n",
        "    print(f\"Found {len(fake_videos)} fake videos in {fake_folder}\")\n",
        "\n",
        "    # Find all .mp4 files in the real folder\n",
        "    real_videos = glob.glob(os.path.join(real_folder, '*.mp4'))\n",
        "    print(f\"Found {len(real_videos)} real videos in {real_folder}\")\n",
        "\n",
        "    # Raise an error if no videos are found\n",
        "    if len(fake_videos) == 0 and len(real_videos) == 0:\n",
        "        raise ValueError(f\"No videos found in {fake_folder} or {real_folder}. Please check the paths.\")\n",
        "\n",
        "    # Combine fake and real video paths\n",
        "    video_files = fake_videos + real_videos\n",
        "    # Create labels (0 for fake, 1 for real)\n",
        "    labels = [0] * len(fake_videos) + [1] * len(real_videos)\n",
        "\n",
        "    # Shuffle the combined list\n",
        "    combined = list(zip(video_files, labels))\n",
        "    random.shuffle(combined)\n",
        "    if len(combined) == 0:\n",
        "        raise ValueError(\"No videos found after combining fake and real folders.\")\n",
        "    video_files[:], labels[:] = zip(*combined)\n",
        "\n",
        "    return video_files, labels"
      ],
      "metadata": {
        "id": "8ZDJfJWoHDlg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to fake and real video folders\n",
        "fake_folder = '/content/drive/MyDrive/Colab Notebooks/fake1.1'\n",
        "real_folder = '/content/drive/MyDrive/Colab Notebooks/real1.1'\n",
        "\n",
        "# Load video paths and labels\n",
        "video_files, labels = load_videos_and_labels(fake_folder, real_folder)\n",
        "\n",
        "# Limit the number of videos to 300 (optional)\n",
        "video_files = video_files[:40000]\n",
        "labels = labels[:40000]\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_ratio = 0.7\n",
        "split_idx = int(train_ratio * len(video_files))\n",
        "train_videos, valid_videos = video_files[:split_idx], video_files[split_idx:]\n",
        "train_labels, valid_labels = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "# Print some info\n",
        "print(f\"Total videos: {len(video_files)}\")\n",
        "print(f\"Training videos: {len(train_videos)}\")\n",
        "print(f\"Validation videos: {len(valid_videos)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY5kWMPrHHEk",
        "outputId": "1b0af84c-097f-4656-938e-3c59741cffa9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17525 fake videos in /content/drive/MyDrive/Colab Notebooks/fake1.1\n",
            "Found 0 real videos in /content/drive/MyDrive/Colab Notebooks/real1.1\n",
            "Total videos: 17525\n",
            "Training videos: 12267\n",
            "Validation videos: 5258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample_minority_class(video_files, labels):\n",
        "    # Count the number of fake and real videos\n",
        "    fake_count = labels.count(0)  # Number of fake videos\n",
        "    real_count = labels.count(1)  # Number of real videos\n",
        "\n",
        "    # Find the indices of real videos\n",
        "    real_indices = [i for i, label in enumerate(labels) if label == 1]\n",
        "\n",
        "    # Calculate how many real videos need to be duplicated\n",
        "    oversample_factor = fake_count // real_count  # How many times to duplicate real videos\n",
        "    oversampled_real_indices = real_indices * oversample_factor  # Duplicate real video indices\n",
        "\n",
        "    # Add the oversampled real videos to the dataset\n",
        "    oversampled_video_files = video_files + [video_files[i] for i in oversampled_real_indices]\n",
        "    oversampled_labels = labels + [labels[i] for i in oversampled_real_indices]\n",
        "\n",
        "    return oversampled_video_files, oversampled_labels\n",
        "\n",
        "# Apply oversampling\n",
        "video_files, labels = oversample_minority_class(video_files, labels)\n",
        "\n",
        "# Verify the balance\n",
        "print(f\"After oversampling: Fake videos = {labels.count(0)}, Real videos = {labels.count(1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "RRf8_m4EHKrs",
        "outputId": "c9fecdad-b4a9-48c0-dd5c-6b619d12ae14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "integer division or modulo by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c111604971d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Apply oversampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mvideo_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moversample_minority_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Verify the balance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-c111604971d3>\u001b[0m in \u001b[0;36moversample_minority_class\u001b[0;34m(video_files, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Calculate how many real videos need to be duplicated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moversample_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_count\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mreal_count\u001b[0m  \u001b[0;31m# How many times to duplicate real videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moversampled_real_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_indices\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moversample_factor\u001b[0m  \u001b[0;31m# Duplicate real video indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample_minority_class(video_files, labels):\n",
        "    # Count the number of fake and real videos\n",
        "    fake_count = labels.count(0)  # Number of fake videos\n",
        "    real_count = labels.count(1)  # Number of real videos\n",
        "\n",
        "    # Find the indices of fake videos\n",
        "    fake_indices = [i for i, label in enumerate(labels) if label == 0]\n",
        "\n",
        "    # Calculate how many fake videos need to be duplicated\n",
        "    oversample_factor = real_count // fake_count\n",
        "    oversampled_fake_indices = fake_indices * oversample_factor\n",
        "\n",
        "    # Add the oversampled fake videos to the dataset\n",
        "    oversampled_video_files = video_files + [video_files[i] for i in oversampled_fake_indices]\n",
        "    oversampled_labels = labels + [labels[i] for i in oversampled_fake_indices]\n",
        "\n",
        "    return oversampled_video_files, oversampled_labels\n",
        "\n",
        "# Apply oversampling\n",
        "video_files, labels = oversample_minority_class(video_files, labels)\n",
        "\n",
        "# Verify the balance\n",
        "print(f\"After oversampling: Fake videos = {labels.count(0)}, Real videos = {labels.count(1)}\")"
      ],
      "metadata": {
        "id": "FwNjmsNaHYoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "# Load video files and labels\n",
        "video_files, labels = load_videos_and_labels(fake_folder, real_folder)\n",
        "\n",
        "# Limit the number of videos to avoid memory overload\n",
        "video_files = video_files[:40000]  # Adjust this number based on your memory capacity\n",
        "labels = labels[:40000]\n",
        "\n",
        "# Apply oversampling to balance the dataset\n",
        "#video_files, labels = oversample_minority_class(video_files, labels)\n",
        "\n",
        "# Verify the balance\n",
        "print(f\"After oversampling: Fake videos = {labels.count(0)}, Real videos = {labels.count(1)}\")\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_ratio = 0.7\n",
        "split_idx = int(train_ratio * len(video_files))\n",
        "train_videos, valid_videos = video_files[:split_idx], video_files[split_idx:]\n",
        "train_labels, valid_labels = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "# Define transforms\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_data = VideoDataset(train_videos, train_labels, sequence_length=10, transform=train_transforms)\n",
        "val_data = VideoDataset(valid_videos, valid_labels, sequence_length=10, transform=train_transforms)\n",
        "\n",
        "# Create DataLoaders with smaller batch size\n",
        "batch_size = 8  # Reduce batch size to save memory\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Get the TPU device\n",
        "device = xm.xla_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load data onto TPU\n",
        "train_loader = pl.MpDeviceLoader(train_loader, device)\n",
        "val_loader = pl.MpDeviceLoader(val_loader, device)\n",
        "\n",
        "# Calculate class weights\n",
        "class_counts = np.bincount(labels)\n",
        "class_weights = 1. / class_counts\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "# Define the loss function with class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
        "\n",
        "# Verify class weights\n",
        "print(f\"Class weights: {class_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKfiza2AHb4m",
        "outputId": "8bb8c7ef-11d1-498d-e162-f0d074eebeb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17525 fake videos in /content/drive/MyDrive/Colab Notebooks/fake1.1\n",
            "Found 54786 real videos in /content/drive/MyDrive/Colab Notebooks/real1.1\n",
            "After oversampling: Fake videos = 9658, Real videos = 30342\n",
            "Using device: xla:0\n",
            "Class weights: tensor([1.0354e-04, 3.2958e-05], device='xla:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoints init"
      ],
      "metadata": {
        "id": "AqSLbjGGHlPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define checkpoint directory (save checkpoints to Google Drive for persistence)\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints1.4\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Hs3N9WwkHr40"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch, model, optimizer, train_loss, train_accuracy, val_loss, val_accuracy, checkpoint_dir):\n",
        "    \"\"\"Save a checkpoint of the model and optimizer.\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_accuracy\n",
        "    }\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "    xm.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} to {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "8H9ne0Q3Hwm4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    \"\"\"Load a checkpoint to resume training.\"\"\"\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        # Use torch.load to load the checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path)  # Changed from xm.load to torch.load\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "        return start_epoch\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting from scratch.\")\n",
        "        return 1  # Start from epoch 1"
      ],
      "metadata": {
        "id": "Nb0YW9CkH5dO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model & Training"
      ],
      "metadata": {
        "id": "Rq5ww9_YINT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
        "        super(Model, self).__init__()\n",
        "        model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size, seq_length, 2048)\n",
        "        x_lstm, _ = self.lstm(x, None)\n",
        "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1)))"
      ],
      "metadata": {
        "id": "E_IwsyEVIUZZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "def train_epochs(epoch, num_epochs, train_loader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])  # Use ParallelLoader for TPU\n",
        "    for batch_idx, (data, target) in enumerate(para_loader.per_device_loader(device)):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        _, output = model(data)\n",
        "\n",
        "        # Debug: Print model output shape\n",
        "        print(f\"Model output shape: {output.shape}\")\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)  # Use xm.optimizer_step for TPU\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_samples += target.size(0)\n",
        "        total_correct += (predicted == target).sum().item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def test(epoch, model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    predicted_probs = []  # Store predicted probabilities\n",
        "\n",
        "    para_loader = pl.ParallelLoader(valid_loader, [device])  # Use ParallelLoader for TPU\n",
        "    with torch.no_grad():\n",
        "        for data, target in para_loader.per_device_loader(device):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, output = model(data)  # Assuming model returns (features, output)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predicted probabilities\n",
        "            probs = torch.softmax(output, dim=1)  # Apply softmax to get probabilities\n",
        "            predicted_probs.extend(probs.cpu().numpy())  # Store probabilities\n",
        "\n",
        "            # Get predicted labels\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total_samples += target.size(0)\n",
        "            total_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Store true and predicted labels\n",
        "            true_labels.extend(target.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = total_loss / len(valid_loader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Validation Epoch [{epoch}], Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Calculate ROC-AUC score\n",
        "    roc_auc = roc_auc_score(true_labels, np.array(predicted_probs)[:, 1])  # Use probabilities for class 1\n",
        "    print(f\"Validation Epoch [{epoch}], ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return true_labels, predicted_labels, predicted_probs, avg_loss, accuracy, roc_auc"
      ],
      "metadata": {
        "id": "9yyRFHJSIfxc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(rank):\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define checkpoint directory\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/checkpoints1.4\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Create DataLoaders with smaller batch size\n",
        "    batch_size = 4  # Reduce batch size to save memory\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    valid_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Define model, optimizer, and loss function\n",
        "    model = Model(2).to(device)\n",
        "    lr = 1e-5\n",
        "    num_epochs = 100\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Load checkpoint if it exists\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_62.pth\")  # Example path\n",
        "    start_epoch = load_checkpoint(checkpoint_path, model, optimizer)\n",
        "\n",
        "    # Lists to store metrics\n",
        "    train_loss_avg = []\n",
        "    train_accuracy = []\n",
        "    test_loss_avg = []\n",
        "    test_accuracy = []\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        print(f\"Starting Epoch {epoch}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        l, acc = train_epochs(epoch, num_epochs, train_loader, model, criterion, optimizer, device)\n",
        "        train_loss_avg.append(l)\n",
        "        train_accuracy.append(acc)\n",
        "\n",
        "        # Validate after each epoch\n",
        "        true, pred, predicted_probs, tl, t_acc, roc_auc = test(epoch, model, valid_loader, criterion, device)\n",
        "        test_loss_avg.append(tl)\n",
        "        test_accuracy.append(t_acc)\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if epoch % 2 == 0:\n",
        "            save_checkpoint(epoch, model, optimizer, l, acc, tl, t_acc, checkpoint_dir)\n",
        "\n",
        "        # Save the best model based on validation accuracy\n",
        "        if t_acc > best_accuracy:\n",
        "            best_accuracy = t_acc\n",
        "            best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "            xm.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Best model saved at epoch {epoch} with validation accuracy {t_acc:.4f}\")\n",
        "\n",
        "    # After training completes, save the final model and training metrics\n",
        "    final_model_path = os.path.join(checkpoint_dir, 'final_model.pth')\n",
        "    final_metrics_path = os.path.join(checkpoint_dir, 'training_metrics.pth')\n",
        "\n",
        "    # Save final model\n",
        "    xm.save(model.state_dict(), final_model_path)\n",
        "\n",
        "    # Save comprehensive training information\n",
        "    torch.save({\n",
        "        'epoch': num_epochs,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss_history': train_loss_avg,\n",
        "        'train_accuracy_history': train_accuracy,\n",
        "        'val_loss_history': test_loss_avg,\n",
        "        'val_accuracy_history': test_accuracy,\n",
        "        'best_accuracy': best_accuracy,\n",
        "        'roc_auc': roc_auc,\n",
        "        'true_labels': true,\n",
        "        'predicted_labels': pred,\n",
        "        'predicted_probs': predicted_probs\n",
        "    }, final_metrics_path)\n",
        "\n",
        "    print(f\"Final model saved to {final_model_path}\")\n",
        "    print(f\"Training metrics saved to {final_metrics_path}\")\n",
        "\n",
        "    # Plot results and confusion matrix\n",
        "    if rank == 0:\n",
        "        plot_loss(train_loss_avg, test_loss_avg, len(train_loss_avg))\n",
        "        plot_accuracy(train_accuracy, test_accuracy, len(train_accuracy))\n",
        "        print_confusion_matrix(true, pred)\n",
        "\n",
        "        # Plot ROC curve if needed\n",
        "        fpr, tpr, thresholds = roc_curve(true, np.array(predicted_probs)[:, 1])\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
        "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Guess\")\n",
        "        plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "        plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "        plt.title(\"ROC Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Vr4PtyFFIXQ4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots & Metrics"
      ],
      "metadata": {
        "id": "xZqNxy6bIirs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(train_loss, test_loss, epochs):\n",
        "    plt.plot(range(1, epochs + 1), train_loss, label='Train Loss')\n",
        "    plt.plot(range(1, epochs + 1), test_loss, label='Validation Loss')\n",
        "    plt.title('Loss vs Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train_accuracy, test_accuracy, epochs):\n",
        "    plt.plot(range(1, epochs + 1), train_accuracy, label='Train Accuracy')\n",
        "    plt.plot(range(1, epochs + 1), test_accuracy, label='Validation Accuracy')\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def print_confusion_matrix(true, pred):\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sn.heatmap(cm, annot=True, fmt='d')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Truth')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "k0_vbjjBI1tS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume you want to test after the last epoch of training\n",
        "# Replace this with the actual number of epochs you trained for\n",
        "num_epochs = 100\n",
        "\n",
        "# After training, validate the model\n",
        "true_labels, predicted_labels, predicted_probs, val_loss, val_accuracy, roc_auc = test(\n",
        "    num_epochs, model, val_loader, criterion, device # Pass num_epochs instead of epoch\n",
        ")\n",
        "\n",
        "# Print ROC-AUC score\n",
        "print(f\"Final ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(true_labels, np.array(predicted_probs)[:, 1])\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RkUAaIegI5UH",
        "outputId": "2f7e69a5-aa49-46d8-b4ce-5bf3fc274e28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8785104ab19a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# After training, validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m true_labels, predicted_labels, predicted_probs, val_loss, val_accuracy, roc_auc = test(\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;31m# Pass num_epochs instead of epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "KjdeUgRHI8zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xmp.spawn(train_fn, nprocs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "skHG40uTJAZ3",
        "outputId": "d6bbc78f-9968-4cac-b2d1-738c92d1a18b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: xla:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
            "100%|██████████| 95.8M/95.8M [00:00<00:00, 222MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from epoch 62\n",
            "Starting Epoch 63\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/W015_light_uniform_angry_camera_left.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/aug_1_115_M123.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/M139_light_right_happy_camera_leftfront.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/aug_1_308_W101_1.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/W036_light_rightdown_surprise_camera_right.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/M024_light_up_happy_camera_left.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/aug_1_772_W028_1.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/aug_0_W133_light_down_disgust_camera_leftfront.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/M117_light_rightup_neutral_camera_right.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/M115_light_leftdown_surprise_camera_rightfront.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/441_W023_1.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/W100_BlendShape_extra_light_rightdown_camera_right.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/aug_2_id10_id11_0006.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/aug_2_W116_light_leftdown_fear_camera_down.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/aug_2_071_M119.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/W014_light_right_contempt_camera_left.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/M014_light_uniform_fear_camera_left.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/M120_light_rightup_sad_camera_rightfront.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/real1.1/W029_light_rightup_surprise_camera_right.mp4\n",
            "Processing video: /content/drive/MyDrive/Colab Notebooks/fake1.1/aug_0_id17_id6_0002.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-87a886e43231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpjrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_run_singleprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m_run_singleprocess\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m   \"\"\"\n\u001b[1;32m     96\u001b[0m   \u001b[0minitialize_singleprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_ordinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3a90b2a0d520>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(rank)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtrain_loss_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-d9b804964ddc>\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(epoch, num_epochs, train_loader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpara_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParallelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use ParallelLoader for TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batches_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36mnext_item\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mdqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/utils/keyd_queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_write\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m       \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "k7SY318TJDfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Define the EXACT same model class used during training\n",
        "class VideoModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(VideoModel, self).__init__()\n",
        "        # Use the same backbone name as during training\n",
        "        backbone = models.resnext50_32x4d(weights=None)\n",
        "        self.model = nn.Sequential(*list(backbone.children())[:-2])  # Note: using 'model' not 'backbone'\n",
        "        self.lstm = nn.LSTM(2048, 2048, 1, bidirectional=False)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size, seq_length, 2048)\n",
        "        x_lstm, _ = self.lstm(x)\n",
        "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1)))\n",
        "\n",
        "# 2. Initialize TPU\n",
        "device = xm.xla_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 3. Load model with proper key mapping\n",
        "model = VideoModel().to(device)\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load('/content/drive/MyDrive/model testing/92ACC.pth',\n",
        "                       map_location='cpu')\n",
        "\n",
        "# Fix key mismatches by removing 'module.' prefix if present\n",
        "state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "new_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "    name = k.replace('module.', '')  # Remove 'module.' if using DataParallel\n",
        "    new_state_dict[name] = v\n",
        "\n",
        "model.load_state_dict(new_state_dict, strict=False)  # strict=False to ignore non-matching keys\n",
        "model.eval()\n",
        "\n",
        "# 4. Define transforms (must match training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: Image.fromarray(x) if isinstance(x, np.ndarray) else x),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 5. Prediction function with TPU support\n",
        "def predict_video(video_path, max_frames=2000000):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened() and len(frames) < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(transform(frame))\n",
        "\n",
        "    if not frames:\n",
        "        raise ValueError(\"No frames could be read from the video\")\n",
        "\n",
        "    frames = torch.stack(frames).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, output = model(frames)\n",
        "        proba = torch.softmax(output, dim=1)\n",
        "        pred = proba.argmax().item()\n",
        "        confidence = proba[0][pred].item()\n",
        "\n",
        "    return \"Real\" if pred == 1 else \"Fake\", confidence\n",
        "\n",
        "# 6. Test with video upload\n",
        "print(\"Please upload a video file...\")\n",
        "uploaded = files.upload()\n",
        "video_file = next(iter(uploaded))\n",
        "prediction, confidence = predict_video(video_file)\n",
        "print(f\"Prediction: {prediction} (Confidence: {confidence:.2%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "H-fnUwhtJG3a",
        "outputId": "a11daf2c-009c-4867-81e6-e035302a6149"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: xla:0\n",
            "Please upload a video file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4c70e6b8-d2b4-44b5-94ea-6533f653e410\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4c70e6b8-d2b4-44b5-94ea-6533f653e410\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 0ABCBZ0CWN.jpg to 0ABCBZ0CWN.jpg\n",
            "Prediction: Real (Confidence: 100.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except:\n",
        "    print(\"Drive already mounted\")\n",
        "\n",
        "# Define the model class exactly as trained\n",
        "class VideoModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(VideoModel, self).__init__()\n",
        "        backbone = models.resnext50_32x4d(weights=None)\n",
        "        self.model = nn.Sequential(*list(backbone.children())[:-2])\n",
        "        self.lstm = nn.LSTM(2048, 2048, 1, bidirectional=False)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(2048, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size, seq_length, 2048)\n",
        "        x_lstm, _ = self.lstm(x)\n",
        "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1)))\n",
        "\n",
        "# Initialize TPU\n",
        "print(\"Initializing TPU...\")\n",
        "try:\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Using device: {device}\")\n",
        "except:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"TPU initialization failed, falling back to: {device}\")\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: Image.fromarray(x) if isinstance(x, np.ndarray) else x),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_model(model_path):\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    model = VideoModel()\n",
        "\n",
        "    # Load checkpoint\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "        # Fix key mismatches by removing 'module.' prefix if present\n",
        "        state_dict = checkpoint['state_dict'] if isinstance(checkpoint, dict) and 'state_dict' in checkpoint else checkpoint\n",
        "        new_state_dict = {}\n",
        "        for k, v in state_dict.items():\n",
        "            name = k.replace('module.', '')  # Remove 'module.' if using DataParallel\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict, strict=False)\n",
        "        print(\"Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_frames(video_path, max_frames=100, uniform_sampling=True):\n",
        "    \"\"\"Extract frames with intelligent sampling for better representation\"\"\"\n",
        "    frames = []\n",
        "\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Warning: Could not open {video_path}\")\n",
        "            return frames\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        if total_frames <= 0:\n",
        "            print(f\"Warning: Invalid frame count for {video_path}\")\n",
        "            return frames\n",
        "\n",
        "        # Select frames based on video length\n",
        "        if uniform_sampling and total_frames > max_frames:\n",
        "            # Sample frames uniformly across the video\n",
        "            indices = np.linspace(0, total_frames-1, max_frames, dtype=int)\n",
        "        else:\n",
        "            # Take sequential frames up to max_frames\n",
        "            indices = range(min(total_frames, max_frames))\n",
        "\n",
        "        # Extract the frames\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                # Convert to RGB and apply transform\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame_tensor = transform(frame)\n",
        "                frames.append(frame_tensor)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting frames from {video_path}: {e}\")\n",
        "\n",
        "    return frames\n",
        "\n",
        "def predict_video_with_batching(video_path, model, frame_batch_size=32):\n",
        "    \"\"\"Process video with memory-efficient frame batching\"\"\"\n",
        "    try:\n",
        "        # Extract frames\n",
        "        frames = extract_frames(video_path)\n",
        "\n",
        "        if not frames:\n",
        "            return \"Error\", 0.0, \"No valid frames extracted\"\n",
        "\n",
        "        num_frames = len(frames)\n",
        "\n",
        "        # Process frames in smaller batches to manage memory\n",
        "        predictions = []\n",
        "        confidences = []\n",
        "\n",
        "        # Split frames into manageable batches\n",
        "        for i in range(0, num_frames, frame_batch_size):\n",
        "            end_idx = min(i + frame_batch_size, num_frames)\n",
        "            batch_frames = frames[i:end_idx]\n",
        "\n",
        "            # Process this batch\n",
        "            batch_tensor = torch.stack(batch_frames).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, output = model(batch_tensor)\n",
        "                proba = torch.softmax(output, dim=1)\n",
        "\n",
        "                # Store results from this batch\n",
        "                batch_pred = proba.argmax(dim=1).item()\n",
        "                batch_conf = proba[0][batch_pred].item()\n",
        "\n",
        "                predictions.append(batch_pred)\n",
        "                confidences.append(batch_conf)\n",
        "\n",
        "        # Combine batch results (majority voting)\n",
        "        final_pred = 1 if sum(pred == 1 for pred in predictions) > len(predictions) / 2 else 0\n",
        "        avg_conf = sum(confidences) / len(confidences)\n",
        "\n",
        "        result = \"Real\" if final_pred == 1 else \"Fake\"\n",
        "        return result, avg_conf, f\"Processed {num_frames} frames\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"Error\", 0.0, str(e)\n",
        "\n",
        "def batch_test_videos(real_dir, fake_dir, model, results_csv=\"model_evaluation_results.csv\"):\n",
        "    \"\"\"Test all videos in directories and produce evaluation metrics\"\"\"\n",
        "    print(f\"Starting batch testing from directories:\\nReal: {real_dir}\\nFake: {fake_dir}\")\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(real_dir):\n",
        "        print(f\"Error: Directory {real_dir} not found\")\n",
        "        return None\n",
        "    if not os.path.exists(fake_dir):\n",
        "        print(f\"Error: Directory {fake_dir} not found\")\n",
        "        return None\n",
        "\n",
        "    results = []\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    confidences = []\n",
        "    file_paths = []\n",
        "\n",
        "    # Process real videos\n",
        "    if os.path.exists(real_dir):\n",
        "        print(\"Processing REAL videos...\")\n",
        "        for filename in tqdm(os.listdir(real_dir)):\n",
        "            if filename.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
        "                file_path = os.path.join(real_dir, filename)\n",
        "                prediction, confidence, status = predict_video_with_batching(file_path, model)\n",
        "\n",
        "                results.append({\n",
        "                    'filename': filename,\n",
        "                    'true_class': 'Real',\n",
        "                    'predicted_class': prediction,\n",
        "                    'confidence': confidence,\n",
        "                    'status': status\n",
        "                })\n",
        "\n",
        "                true_labels.append(1)  # 1 for real\n",
        "                pred_labels.append(1 if prediction == \"Real\" else 0)\n",
        "                confidences.append(confidence)\n",
        "                file_paths.append(file_path)\n",
        "\n",
        "    # Process fake videos\n",
        "    if os.path.exists(fake_dir):\n",
        "        print(\"Processing FAKE videos...\")\n",
        "        for filename in tqdm(os.listdir(fake_dir)):\n",
        "            if filename.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
        "                file_path = os.path.join(fake_dir, filename)\n",
        "                prediction, confidence, status = predict_video_with_batching(file_path, model)\n",
        "\n",
        "                results.append({\n",
        "                    'filename': filename,\n",
        "                    'true_class': 'Fake',\n",
        "                    'predicted_class': prediction,\n",
        "                    'confidence': confidence,\n",
        "                    'status': status\n",
        "                })\n",
        "\n",
        "                true_labels.append(0)  # 0 for fake\n",
        "                pred_labels.append(1 if prediction == \"Real\" else 0)\n",
        "                confidences.append(confidence)\n",
        "                file_paths.append(file_path)\n",
        "\n",
        "    # Save detailed results to CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(results_csv, index=False)\n",
        "    print(f\"Detailed results saved to {results_csv}\")\n",
        "\n",
        "    # Return data for metrics calculation\n",
        "    return true_labels, pred_labels, confidences, file_paths, df\n",
        "\n",
        "def calculate_metrics(true_labels, pred_labels, confidences, file_paths, results_df):\n",
        "    \"\"\"Calculate and visualize performance metrics\"\"\"\n",
        "    if not true_labels or len(true_labels) != len(pred_labels):\n",
        "        print(\"Error: Invalid or empty label data\")\n",
        "        return\n",
        "\n",
        "    # Convert to numpy arrays for analysis\n",
        "    y_true = np.array(true_labels)\n",
        "    y_pred = np.array(pred_labels)\n",
        "    y_scores = np.array(confidences)\n",
        "\n",
        "    # 1. Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Fake', 'Real'],\n",
        "                yticklabels=['Fake', 'Real'])\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Classification Report\n",
        "    report = classification_report(y_true, y_pred, target_names=['Fake', 'Real'], output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report_df)\n",
        "    report_df.to_csv('classification_report.csv')\n",
        "\n",
        "    # 3. ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('roc_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('precision_recall_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Analysis of Misclassifications\n",
        "    misclassified = results_df[results_df['true_class'] != results_df['predicted_class']]\n",
        "    print(\"\\nSample of Misclassified Videos:\")\n",
        "    print(misclassified.head(10))\n",
        "\n",
        "    # 6. Confidence Distribution by True Class\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(data=results_df, x='confidence', hue='true_class',\n",
        "                 element='step', stat='density', common_norm=False, bins=20)\n",
        "    plt.title('Confidence Score Distribution by True Class')\n",
        "    plt.xlabel('Confidence Score')\n",
        "    plt.ylabel('Density')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confidence_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 7. Confidence Threshold Analysis\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "    accuracy_scores = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Adjust predictions based on confidence threshold\n",
        "        adjusted_preds = [(1 if confidence >= threshold else 0) if pred == 1 else\n",
        "                         (0 if confidence >= threshold else 1)\n",
        "                         for pred, confidence in zip(y_pred, y_scores)]\n",
        "\n",
        "        # Calculate accuracy at this threshold\n",
        "        accuracy = np.mean(np.array(adjusted_preds) == y_true)\n",
        "        accuracy_scores.append(accuracy)\n",
        "\n",
        "    # Find optimal threshold\n",
        "    optimal_idx = np.argmax(accuracy_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    optimal_accuracy = accuracy_scores[optimal_idx]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(thresholds, accuracy_scores, 'b-')\n",
        "    plt.axvline(x=optimal_threshold, color='r', linestyle='--',\n",
        "               label=f'Optimal threshold: {optimal_threshold:.2f}\\nAccuracy: {optimal_accuracy:.2f}')\n",
        "    plt.xlabel('Confidence Threshold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Impact of Confidence Threshold on Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('threshold_analysis.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 8. Summary table\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    summary = {\n",
        "        'Total Videos': len(y_true),\n",
        "        'Correct Predictions': np.sum(y_true == y_pred),\n",
        "        'True Positive': tp,\n",
        "        'True Negative': tn,\n",
        "        'False Positive': fp,\n",
        "        'False Negative': fn,\n",
        "        'Accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
        "        'Precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
        "        'Recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
        "        'F1 Score': report['macro avg']['f1-score'],\n",
        "        'AUC-ROC': roc_auc,\n",
        "        'AUC-PR': pr_auc,\n",
        "        'Optimal Confidence Threshold': optimal_threshold\n",
        "    }\n",
        "\n",
        "    summary_df = pd.DataFrame({k: [v] for k, v in summary.items()}).T\n",
        "    summary_df.columns = ['Value']\n",
        "    print(\"\\nPerformance Summary:\")\n",
        "    print(summary_df)\n",
        "    summary_df.to_csv('performance_summary.csv')\n",
        "\n",
        "    # Return summary metrics dictionary\n",
        "    return summary\n",
        "\n",
        "# MAIN EXECUTION\n",
        "\n",
        "# 1. Load the model\n",
        "model_path = '/content/drive/MyDrive/model testing/92ACC.pth'\n",
        "model = load_model(model_path)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Failed to load model. Exiting.\")\n",
        "else:\n",
        "    # 2. Define test directories (using your provided paths)\n",
        "    real_dir = '/content/drive/MyDrive/model testing/archive1.zip (Unzipped Files)/real_vs_fake/real-vs-fake/test/real'\n",
        "    fake_dir = '/content/drive/MyDrive/model testing/archive1.zip (Unzipped Files)/real_vs_fake/real-vs-fake/test/fake'\n",
        "\n",
        "    # 3. Run batch testing\n",
        "    true_labels, pred_labels, confidences, file_paths, results_df = batch_test_videos(real_dir, fake_dir, model)\n",
        "\n",
        "    # 4. Calculate and display metrics\n",
        "    if true_labels:\n",
        "        print(\"\\nCalculating performance metrics...\")\n",
        "        metrics = calculate_metrics(true_labels, pred_labels, confidences, file_paths, results_df)\n",
        "\n",
        "        print(\"\\nTesting complete! Visualizations and reports have been saved.\")\n",
        "    else:\n",
        "        print(\"No valid test data was processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqMAXEklNbih",
        "outputId": "6a9dbe85-69c0-422c-b6c6-70fd54c9aaa9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Initializing TPU...\n",
            "Using device: xla:0\n",
            "Loading model from: /content/drive/MyDrive/model testing/92ACC.pth\n",
            "Model loaded successfully!\n",
            "Starting batch testing from directories:\n",
            "Real: /content/drive/MyDrive/model testing/archive1.zip (Unzipped Files)/real_vs_fake/real-vs-fake/test/real\n",
            "Fake: /content/drive/MyDrive/model testing/archive1.zip (Unzipped Files)/real_vs_fake/real-vs-fake/test/fake\n",
            "Processing REAL videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing FAKE videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1971/1971 [00:00<00:00, 2557059.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detailed results saved to model_evaluation_results.csv\n",
            "No valid test data was processed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}